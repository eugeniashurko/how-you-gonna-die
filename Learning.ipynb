{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "In this part, we try to learn the way people die, based on 4 features generated before:\n",
    "* Education (8 bins)\n",
    "* Sex\n",
    "* Race (White, Black, Other)\n",
    "* Martial status (Single, Married, ...)\n",
    "\n",
    "We will try to use some learning models to \"guess\" how people die based one these 4 informations.  \n",
    "Two informaions will be tried to be learned: \n",
    "* The **manner** of death. 7 categories:\n",
    "  * Accident\n",
    "  * Suicide\n",
    "  * Homicide\n",
    "  * Pending investigation\n",
    "  * Could not determine\n",
    "  * Self-Inflicted\n",
    "  * Natural\n",
    "* The **cause** of death (much more precise: 39 categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Manner of death (7 bins)\n",
    "\n",
    "First, we load the features generated by previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pickle import load\n",
    "from time import time\n",
    "\n",
    "features = load(open(\"features4.pickle\", \"rb\"))\n",
    "y_all = load(open(\"manner.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, we generate the train and test sets, the train set being a random subset of 10000 people from the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "selected = np.random.randint(y_all.shape[0],size=10000)\n",
    "X = features[selected,:]\n",
    "y = y_all[selected]\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these sets, we perform a transformation to perform distances between **categorical** features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "X = enc.fit_transform(X)\n",
    "X_test = enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING....\n",
      "TIME:\n",
      " 5.259270191192627\n",
      "SCORE:\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import cross_validation\n",
    "\n",
    "C = 10\n",
    "clf = OneVsRestClassifier(SVC(kernel='linear', C=C, probability=True), n_jobs=4)\n",
    "\n",
    "print(\"TRAINING....\")\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(X, y)\n",
    "t1 = time()\n",
    "print(\"TIME:\\n\", t1-t0)\n",
    "\n",
    "print(\"SCORE:\")\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING....\n",
      "TIME:\n",
      " 0.0017457008361816406\n",
      "SCORE:\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=300)\n",
    "print(\"TRAINING....\")\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(X, y)\n",
    "t1 = time()\n",
    "print(\"TIME:\\n\", t1-t0)\n",
    "\n",
    "print(\"SCORE:\")\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING....\n",
      "TIME:\n",
      " 0.0060193538665771484\n",
      "SCORE:\n",
      "0.779090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "print(\"TRAINING....\")\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(X.toarray(), y)\n",
    "t1 = time()\n",
    "print(\"TIME:\\n\", t1-t0)\n",
    "\n",
    "\n",
    "print(\"SCORE:\")\n",
    "print(clf.score(X_test.toarray(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the three models give greate results (very high score), but we can see that the linear SVC is much slower than the other two.  \n",
    "For the examples below, the Naive Bayes model is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black man with no education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0936895 ,  0.1109537 ,  0.00976497,  0.2584388 ,  0.00478453,\n",
       "         0.00826412,  0.51410439]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = enc.transform([[1, 0, 3, 0]])\n",
    "clf.predict_proba(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White woman married, with high education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.63399336e-01,   2.22790087e-02,   4.66005052e-03,\n",
       "          2.64268740e-04,   1.02434968e-03,   5.27518901e-04,\n",
       "          8.07845468e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = enc.transform([[6, 1, 1, 1]])\n",
    "clf.predict_proba(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning the cause of death (39 bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = load(open(\"features4.pickle\", \"rb\"))\n",
    "y_all = load(open(\"cause.pickle\", \"rb\"))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "selected = np.random.randint(y_all.shape[0],size=10000)\n",
    "X = features[selected,:]\n",
    "y = y_all[selected]\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "X = enc.fit_transform(X)\n",
    "X_test = enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING....\n",
      "TIME:\n",
      " 0.0033082962036132812\n",
      "SCORE:\n",
      "0.175454545455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=300)\n",
    "print(\"TRAINING....\")\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(X, y)\n",
    "t1 = time()\n",
    "print(\"TIME:\\n\", t1-t0)\n",
    "\n",
    "print(\"SCORE:\")\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for learning these 39 bins, we don't have much information. The error is to small to expect reliable results from the model. We are here at the limit of this dataset. \n",
    "\n",
    "To be able to learn this precise cause of deatch, we must have access to more information about these people like medical details, geographic position, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
